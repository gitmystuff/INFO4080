{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitmystuff/INFO4080/blob/main/Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformers"
      ],
      "metadata": {
        "id": "ghkm4G_Kan4q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Abstract Ideas\n",
        "\n",
        "* https://energywavetheory.com/equations/emc2/\n",
        "* https://en.wikipedia.org/wiki/Musical_note\n",
        "* https://www.dailyarthub.com/shop/objects/free-science-symbols-clip-art-set/\n",
        "* https://decodingdatascience.com/attention-is-all-you-need-transforming-the-landscape-of-machine-learning/"
      ],
      "metadata": {
        "id": "bpHtLdla4iuv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Networks"
      ],
      "metadata": {
        "id": "fjxWYKZi4kmb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ANNs\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Neural_network_(machine_learning)"
      ],
      "metadata": {
        "id": "z1H2d2mCz7bd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNNs\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Convolutional_neural_network\n",
        "* https://www.ibm.com/topics/convolutional-neural-networks\n",
        "* https://ieeexplore.ieee.org/document/726791"
      ],
      "metadata": {
        "id": "BGznBzzI0JQl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNNs\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Recurrent_neural_network\n",
        "* x in time t, is input to RNN unit, and outputs a hidden vector, or state, at time t\n",
        "\n",
        "#### Types of RNNs\n",
        "\n",
        "* https://www.educative.io/answers/what-are-the-types-of-rnn"
      ],
      "metadata": {
        "id": "9CAImaovwnxz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM\n",
        "\n",
        "https://en.wikipedia.org/wiki/Long_short-term_memory\n",
        "\n",
        "* Cell\n",
        "* Input Gate\n",
        "* Output Gate\n",
        "* Forget Gate\n",
        "* Activation Function (represented by $\\sigma$)\n",
        "* https://www.researchgate.net/figure/Structure-of-the-LSTM-cell-and-equations-that-describe-the-gates-of-an-LSTM-cell_fig5_329362532\n",
        "* The cell state represents the memory of the network, information over time, while the hidden state contains the (new) processed information that is passed to the next time step"
      ],
      "metadata": {
        "id": "ekdR37dO1mGI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Seq2Seq\n",
        "\n",
        "* Encoder / Decoder\n",
        "* https://en.wikipedia.org/wiki/Seq2seq\n",
        "* https://www.researchgate.net/figure/Seq2Seq-model-neural-encoder-and-decoder-32_fig4_334023532\n",
        "* h(t), a compressed representation (vector) of the input (thought vector), is between encoder and decoder\n",
        "* A problem with Seq2Seq is using long passages, long-term dependencies, and remembering context\n",
        "* h(t) always has the same size and can cause a bottle neck"
      ],
      "metadata": {
        "id": "scz3qFHKzJQo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM vs Seq2Seq\n",
        "\n",
        "* Both are many to many models\n",
        "* LSTMs are building blocks\n",
        "* Seq2Seq is an architecture built from encoder LSTM and decoder LSTM, etc."
      ],
      "metadata": {
        "id": "KCm4aNrT3aOU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### All You Need is Attention\n",
        "\n",
        "* For each output token, we want to know which input to pay attention to by using weights\n",
        "* https://theaisummer.com/attention/\n",
        "* Attention layer is a mapping of inputs to outputs\n",
        "\n",
        "#### No More RNN\n",
        "\n",
        "* RNNs are slow because they are sequential\n",
        "* Cannot be parallelized\n",
        "* Vanishing Gradients"
      ],
      "metadata": {
        "id": "m2baafJ837MH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary\n",
        "\n",
        "(RNN, Seq2Seq, Attention, Transformers)\n",
        "\n",
        "* Attention is all you need - https://arxiv.org/abs/1706.03762\n",
        "* The attention mechanism allows the model to focus on different parts of the input text. This helps the model to generate more accurate predictions. This allows words in a text sequence to pay attention to the earlier words in the sequence. This helps to model large text sequences in a correct manner.\n",
        "* This process can be thought of as autoregression (what is the next best word)\n",
        "* Transformers are big and slow but can be done in parallel, unlike RNNs\n",
        "* Provides longer range than LSTM\n",
        "* Created for RNNs (LSTMs), but transformers only use attention\n",
        "* Assign varying levels of importance to different parts of a sentence or text\n",
        "* Capture contextual information effectively\n",
        "* Understand the relationships between words\n",
        "* Extract the most relevant parts of a query while disregarding less important ones\n",
        "* Comprehend pronouns, two-sided words, and related sentences more effectively\n",
        "* Generate coherent answers for queries\n"
      ],
      "metadata": {
        "id": "msorcAozvdJf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLMs - Tokens and Transformers\n",
        "\n",
        "* Completion vs Chat\n",
        "* Chat needs context, memory\n",
        "* LangChain\n",
        "* Transfer Learning: In transfer learning, a machine exploits the knowledge gained from a previous task to improve generalization about another. For example, in training a classifier to predict whether an image contains food, you could use the knowledge it gained during training to recognize drinks. - https://builtin.com/data-science/transfer-learning\n",
        "* HuggingFace Transformers - https://huggingface.co/models\n",
        "\n",
        "Transformer Architecture\n",
        "\n",
        "* https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention\n",
        "\n",
        "Encoding vs Decoding\n",
        "\n",
        "However, the main difference is that encoders are designed to learn embeddings that can be used for various predictive modeling tasks such as classification. In contrast, decoders are designed to generate new texts, for example, answering user queries.\n",
        "\n",
        "https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder\n",
        "\n",
        "Attention\n",
        "\n",
        "* The chicken crossed the road because it was hungry\n",
        "* The chicken crossed the road because it was the way home\n",
        "* What is it?\n",
        "\n",
        "Query, Key, Value\n",
        "\n",
        "* The query vector represents the current focus of the LLM, like the central theme of the sentence. The key and value vectors hold information about each word, like its definition and role in the context.\n",
        "* https://www.thecloudgirl.dev/blog/how-attention-makes-llms-powerful\n",
        "\n",
        "Pipeline\n",
        "\n",
        "<pre>\n",
        "from transformers import pipeline\n",
        "</pre>\n",
        "\n",
        "* Text Processing\n",
        "* Tokenize Data\n",
        "* Model\n",
        "* Numerical Predictions\n",
        "* Post Processing\n",
        "* Human Readable Predictions\n",
        "\n",
        "Highlights\n",
        "\n",
        "* Tokenizer\n",
        "* Sentiment Analysis\n",
        "* Text Generation\n",
        "* Masked Language Modeling\n",
        "* NER\n",
        "* Summarization\n",
        "* Translation\n",
        "* Question and Answer\n",
        "* Zero-Shot Classification\n",
        "\n",
        "Source\n",
        "\n",
        "* https://deeplearningcourses.com/c/data-science-transformers-nlp"
      ],
      "metadata": {
        "id": "nlLlzOuj4XI8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer\n",
        "\n",
        "* Stack of attention layers\n",
        "* Pre trained stack of mappings from Encoder to Decoder\n",
        "* The attention function can be considered a mapping between a query and a set of key-value pairs to an output\n",
        "* Query, Key, Value\n",
        "\n",
        "Sources\n",
        "\n",
        "* https://arxiv.org/abs/1706.03762\n",
        "* https://machinelearningmastery.com/the-transformer-attention-mechanism/\n",
        "* https://www.linkedin.com/pulse/unpacking-query-key-value-transformers-analogy-database-mohamed-nabil/"
      ],
      "metadata": {
        "id": "7szSXEqE8JOj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizer\n",
        "\n",
        "* Different models yield different outputs\n",
        "* Padding and Truncation"
      ],
      "metadata": {
        "id": "SL2mun-HwPEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoTokenizer\n",
        "\n",
        "# model = 'bert-base-uncased'\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model)"
      ],
      "metadata": {
        "id": "AQ1ft-LD4uZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # https://huggingface.co/docs/transformers/en/model_doc/bert\n",
        "# tokenizer('happy friday')"
      ],
      "metadata": {
        "id": "asxBPaIl6rFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer('Antidisestablishmentarianism')\n",
        "# # This 28-letter word refers to a 19th-century political movement that opposed the disestablishment of the Church of England."
      ],
      "metadata": {
        "id": "xshykUoPnlWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokens = tokenizer.tokenize('happy friday')\n",
        "# print(tokens)\n",
        "# ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "# print(ids)\n",
        "# print(tokenizer.decode(ids))\n",
        "# ids = tokenizer.encode('happy friday')\n",
        "# print(ids)\n",
        "# tokenizer.decode(ids)"
      ],
      "metadata": {
        "id": "UbC8QVGc7O38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data = [\n",
        "#     'I like cats.',\n",
        "#     'Do you like cats too?',\n",
        "#     'Wow, those cats like really know how to play groovy music.',\n",
        "# ]\n",
        "# tokenizer(data)"
      ],
      "metadata": {
        "id": "fDLseVFi8sGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sentence1 = \"What is the capital of India?\"\n",
        "# sentence2 = \"new Delhi is the capital.\"\n",
        "\n",
        "# encoded_input = tokenizer(sentence1, sentence2, padding=\"max_length\", max_length=20)\n",
        "# encoded_input"
      ],
      "metadata": {
        "id": "5dguubvAjzJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer.decode(encoded_input['input_ids'])"
      ],
      "metadata": {
        "id": "rVzAFQy3ksDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Attention Mask**\n",
        "\n",
        "* **Purpose:** Tells the model which tokens to pay attention to and which to ignore.\n",
        "* **Different Values:**  You'll see different values (0s instead of all 1s) when you have:\n",
        "* **Padding:** If your input texts have different lengths, they need to be padded to the same length for efficient batch processing.  The `attention_mask` will have 0s for the padded tokens, signaling the model to ignore them.\n",
        "* **Masking for Prediction:** During training (especially in tasks like masked language modeling), some tokens are intentionally masked (hidden) and the model tries to predict them. The `attention_mask` uses 0s to indicate these masked tokens.\n",
        "\n",
        "**Token Type IDs**\n",
        "\n",
        "* **Purpose:**  Distinguishes between different sentences or segments in the input.\n",
        "* **Different Values:**  You'll see different values (1s) when you have:\n",
        "* **Multiple Sentences:** For tasks like question answering or sentence pair classification, you input two sentences. The `token_type_ids` will have 0s for the tokens of the first sentence and 1s for the tokens of the second sentence.\n",
        "\n",
        "**Example**\n",
        "\n",
        "Let's say you have two sentences:\n",
        "\n",
        "* Sentence 1: \"What is the capital of India?\"\n",
        "* Sentence 2: \"New Delhi is the capital.\"\n",
        "\n",
        "Here's how the tokenizer might process them, including padding:\n",
        "\n",
        "```python\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model = 'bert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "\n",
        "sentence1 = \"What is the capital of India?\"\n",
        "sentence2 = \"New Delhi is the capital.\"\n",
        "\n",
        "encoded_input = tokenizer(sentence1, sentence2, padding=\"max_length\", max_length=15)\n",
        "print(encoded_input)\n",
        "```\n",
        "\n",
        "Output (likely similar, but exact IDs might vary):\n",
        "\n",
        "```\n",
        "{\n",
        " 'input_ids': [101, 2054, 2003, 1996, 3231, 1997, 2188, 102, 7328, 2003, 1996, 3231, 102, 0, 0],\n",
        " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
        " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n",
        "}\n",
        "```\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "* **`input_ids`:** The numerical IDs of the tokens from both sentences, including special tokens and padding (0s).\n",
        "* **`token_type_ids`:** 0s for the tokens in the first sentence (\"What is the capital of France?\") and 1s for the second sentence (\"Paris is the capital.\").\n",
        "* **`attention_mask`:** 1s for all the actual tokens and 0s for the two padding tokens at the end.\n",
        "\n",
        "This example shows how `attention_mask` and `token_type_ids` help the model understand the structure and relevant parts of the input text.\n"
      ],
      "metadata": {
        "id": "7539tI8emh7y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentiment Analysis"
      ],
      "metadata": {
        "id": "FAFVOLD0oEfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import pipeline\n",
        "\n",
        "# sa = pipeline('sentiment-analysis')\n",
        "# print(sa('I love Thursdays'))\n",
        "# print(sa('I am sad to see the class end on Thursday night'))\n",
        "# print(sa('I have mixed emotions about Thursdays'))"
      ],
      "metadata": {
        "id": "ZaTAAc6U9r8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This output shows the download progress of different components required for a language model, likely GPT-2 in this case. Here's a breakdown of what each file represents:\n",
        "\n",
        "* **`config.json`:**  This file contains the configuration settings for the model, such as the number of layers, attention heads, hidden units, and other architectural choices. It defines the structure and hyperparameters of the model.\n",
        "\n",
        "* **`model.safetensors`:** This file stores the learned weights (parameters) of the neural network. These weights determine how the model processes and generates text, and they are the result of the model's training on a massive dataset. The \".safetensors\" format is a newer format for saving PyTorch models that focuses on safety and portability.\n",
        "\n",
        "* **`generation_config.json`:** This file contains settings specifically related to text generation with the model. This might include parameters like `max_length`, `temperature` (controlling randomness), and `top_k` (limiting the choices for the next token).\n",
        "\n",
        "* **`tokenizer_config.json`:**  This file holds the configuration for the tokenizer associated with the model. It defines how the tokenizer splits text into tokens, maps tokens to numerical IDs, and handles special tokens.\n",
        "\n",
        "* **`vocab.json`:** This file contains the model's vocabulary, which is a list of all the tokens (words, subwords, or characters) the model knows. Each token is associated with a unique numerical ID.\n",
        "\n",
        "* **`merges.txt`:** This file is used by some tokenizers (like the one for GPT-2) to handle subword tokenization. It defines rules for merging subword units into full words.\n",
        "\n",
        "* **`tokenizer.json`:** This file might contain additional information or metadata related to the tokenizer.\n",
        "\n",
        "**In summary, these downloads represent the essential components that make up a language model:**\n",
        "\n",
        "* The model's architecture (`config.json`)\n",
        "* The learned knowledge (`model.safetensors`)\n",
        "* The text processing rules (`tokenizer_config.json`, `vocab.json`, `merges.txt`, `tokenizer.json`)\n",
        "* The generation parameters (`generation_config.json`)\n",
        "\n",
        "By downloading all these files, you get a complete and ready-to-use language model that can be loaded and used for various natural language processing tasks.\n"
      ],
      "metadata": {
        "id": "SDM8VHVxm_e3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sa(['i really like talking in my sleep',\n",
        "#     'i wish i would stop snoring because I can\\'t hear myself talk and it\\'s driving me cuckoo'])"
      ],
      "metadata": {
        "id": "YG6Qy_S--_Y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Generation"
      ],
      "metadata": {
        "id": "P6yt0s9soMf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# url = 'https://raw.githubusercontent.com/gitmystuff/Datasets/main/alice%20in%20wonderland%20chapter%201.txt'\n",
        "\n",
        "# from transformers import pipeline\n",
        "# import requests\n",
        "\n",
        "# response = requests.get(url)\n",
        "# data = response.text\n",
        "# print(data)"
      ],
      "metadata": {
        "id": "vKaYj_tzL37G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lines = data.splitlines()\n",
        "# lines[2]"
      ],
      "metadata": {
        "id": "USgGwkXx_9l4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gen = pipeline('text-generation', max_length=100)\n",
        "# print(gen(lines[2]))"
      ],
      "metadata": {
        "id": "HD9Tu_OIBZ-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import textwrap\n",
        "\n",
        "# def wrap(x):\n",
        "#   return textwrap.fill(x, replace_whitespace=False, fix_sentence_endings=True)\n",
        "\n",
        "# prompt = 'The book had no pictures or conversations in it, and what is the use of a book, thought Alice without pictures or conversations'\n",
        "# out = gen(prompt, max_length=300)\n",
        "# print(wrap(out[0]['generated_text']))"
      ],
      "metadata": {
        "id": "w1vBsom1AWGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Masked Language Modeling"
      ],
      "metadata": {
        "id": "A90bbvA0oSpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget -nc https://raw.githubusercontent.com/gitmystuff/Datasets/main/news_stories.csv"
      ],
      "metadata": {
        "id": "otN2cXd_os_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# df = pd.read_csv('news_stories.csv')\n",
        "# df.head()"
      ],
      "metadata": {
        "id": "9H6xwYDhD4gR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df = df[df['category'].isin(['POLITICS', 'TRAVEL', 'WEDDINGS', 'EDUCATION', 'SCIENCE'])]\n",
        "# df['category'].value_counts()"
      ],
      "metadata": {
        "id": "bupAMSv8EKSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cats = 'POLITICS'\n",
        "# texts = df[df['category'] == cats]['short_description']\n",
        "# texts = texts.reset_index()\n",
        "# texts.drop('index', axis=1, inplace=True)\n",
        "# texts.head()"
      ],
      "metadata": {
        "id": "U6pT_nN4Fily"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# texts.iloc[3]['short_description']"
      ],
      "metadata": {
        "id": "ycE1NbiNGGbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mlm = pipeline('fill-mask')"
      ],
      "metadata": {
        "id": "xxgeMQZWGr-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mlm('He\\'s not saying <mask> should go if the sexual assault allegations are true. He\\'s telling him to get out now')"
      ],
      "metadata": {
        "id": "ianJHMcvG28t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question Answer"
      ],
      "metadata": {
        "id": "xJQNTlGcuC6Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zero Shot Classification / Learning\n",
        "\n",
        "Zero-shot learning and few-shot learning are both machine learning techniques that use minimal data to teach AI, but they differ in how many examples are provided to the model during training:\n",
        "\n",
        "* **Zero-shot:** The LLM is given **no examples** and has to complete the task based on its general knowledge from training. It's like taking a test without studying!\n",
        "    * Example:  Asking an LLM to translate a sentence from English to French without providing any prior examples of English-French translations.\n",
        "\n",
        "* **One-shot:** The LLM is given **one example** of the task before being asked to do it. It's like being shown how to solve one math problem and then being asked to solve a similar one.\n",
        "    * Example: Showing an LLM one example of a movie review labeled as \"positive\" and then asking it to classify the sentiment of a new movie review.\n",
        "\n",
        "* **Few-shot:** The LLM is given a **small number of examples** (usually 2-10) before being asked to perform the task.  This gives it a bit more context to work with.\n",
        "    * Example: Providing an LLM with a few examples of questions and answers about a specific topic and then asking it to answer a new question on that topic.\n",
        "\n",
        "**Why are these \"shots\" important?**\n",
        "\n",
        "* **Generalization:** They test how well an LLM can generalize its knowledge to new situations and tasks.\n",
        "* **Learning efficiency:**  They assess how quickly an LLM can learn new concepts from limited examples.\n",
        "* **Adaptability:** They evaluate the LLM's ability to adapt to different tasks and domains.\n",
        "\n",
        "**Beyond the basics:**\n",
        "\n",
        "While zero-shot, one-shot, and few-shot are the most common types of \"shots,\" there are other variations:\n",
        "\n",
        "* **Multi-shot:**  Providing more than a few examples, but still a relatively small number.\n",
        "* **In-context learning:**  A broader term that encompasses few-shot learning and refers to the LLM's ability to learn from examples within the prompt itself.\n",
        "\n",
        "The type of \"shot\" used depends on the specific task, the capabilities of the LLM, and the desired level of performance.\n"
      ],
      "metadata": {
        "id": "BlsmgdjOum8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import pipeline\n",
        "\n",
        "# zsc = pipeline('zero-shot-classification')"
      ],
      "metadata": {
        "id": "noWOhdQOIRwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# zsc('This is really cool', candidate_labels=['positive', 'negative'])"
      ],
      "metadata": {
        "id": "B8cw1l4VIrN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# idx = 20\n",
        "# cats = list(set(df['category']))\n",
        "# print(cats)\n",
        "# print(df.iloc[idx]['short_description'])\n",
        "# print(df.iloc[idx]['category'])"
      ],
      "metadata": {
        "id": "deWNBvA7JFjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# zsc(df.iloc[idx]['short_description'], candidate_labels=cats)"
      ],
      "metadata": {
        "id": "1gD2lRbTKGv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers langchain langchain-community"
      ],
      "metadata": {
        "id": "b_5OCW8CG-kK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install langchain_huggingface"
      ],
      "metadata": {
        "id": "LfwXSMnxQovN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_huggingface import HuggingFacePipeline\n",
        "\n",
        "# llm = HuggingFacePipeline.from_model_id(\n",
        "#     model_id=\"gpt2\",\n",
        "#     task=\"text-generation\",\n",
        "#     pipeline_kwargs={\n",
        "#         \"max_new_tokens\": 100,\n",
        "#         \"top_k\": 50,\n",
        "#         \"top_p\": 0.95,\n",
        "#         \"temperature\": 0.1,\n",
        "#     },\n",
        "# )\n",
        "\n",
        "# llm.invoke(\"Data Science is\")"
      ],
      "metadata": {
        "id": "nDRNADjeQodf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You're looking at code that creates a Hugging Face language model (LLM) for text generation within LangChain! Let's break down those `pipeline_kwargs`:\n",
        "\n",
        "* **`model_id=\"gpt2\"`:**  This specifies the name of the pre-trained model you want to use from the Hugging Face Model Hub. In this case, it's \"gpt2\", a popular language model.\n",
        "\n",
        "* **`task=\"text-generation\"`:** This tells the `HuggingFacePipeline` that you want to use the model for generating text.\n",
        "\n",
        "Now, let's dive into the `pipeline_kwargs`:\n",
        "\n",
        "* **`max_new_tokens=100`:** This limits the number of new tokens (roughly words or sub-words) that the model will generate in its output. Here, the generated text will be at most 100 tokens long.\n",
        "\n",
        "* **`top_k=50`:** This parameter uses top-k sampling to control the randomness of the generated text.  The model will only consider the 50 most likely tokens when generating the next word. This helps to make the output more focused and coherent.\n",
        "\n",
        "* **`top_p=0.95`:** This parameter uses nucleus sampling (also known as top-p sampling). It selects tokens from the probability distribution until the cumulative probability exceeds the `top_p` value. This dynamically adjusts the number of tokens considered, allowing for more flexibility and potentially more diverse output.\n",
        "\n",
        "* **`temperature=0.1`:** This controls the \"creativity\" or randomness of the generated text. A lower temperature (like 0.1) makes the output more deterministic and focused, while a higher temperature (closer to 1) makes it more random and creative.\n",
        "\n",
        "**In summary:** These `pipeline_kwargs` fine-tune the behavior of the Hugging Face language model, influencing the length, randomness, and diversity of the generated text. They give you control over how the model generates responses, allowing you to tailor it to your specific needs within your LangChain application.\n",
        "\n",
        "**top_k and temperature**\n",
        "\n",
        "`top_k` is another parameter used to control the randomness of language model output, but it works differently than temperature. While temperature scales the entire probability distribution, `top_k` focuses on a specific number of the most likely tokens.\n",
        "\n",
        "Here's how it works:\n",
        "\n",
        "1. **Probability Distribution:** At each step of text generation, the language model calculates a probability distribution over its vocabulary (all the possible words or tokens it knows). This distribution represents how likely each word is to be the next one in the sequence.\n",
        "\n",
        "2. **Selecting Top Tokens:**  `top_k` sampling selects the top `k` most likely tokens from this distribution. For example, if `top_k` is set to 40, only the 40 most probable words will be considered for the next token in the generated text.\n",
        "\n",
        "3. **Resampling:** The probability distribution is then re-normalized over these selected `k` tokens, and the model samples from this narrowed-down distribution.\n",
        "\n",
        "**Effects of `top_k`:**\n",
        "\n",
        "* **Increased Focus:** By limiting the choices to the most likely tokens, `top_k` sampling makes the output more focused and coherent. It prevents the model from considering very low-probability tokens that might lead to nonsensical or irrelevant output.\n",
        "* **Controlled Randomness:** Even though it reduces the choices, `top_k` still allows for randomness within the top `k` tokens. This helps maintain some diversity and creativity in the generated text.\n",
        "\n",
        "**Typical Usage:**\n",
        "\n",
        "* **Smaller `top_k` values (e.g., 10, 20):**  Produce more deterministic and predictable output, suitable for tasks where accuracy is crucial.\n",
        "* **Larger `top_k` values (e.g., 50, 100):** Allow for more diversity and creativity, useful for tasks like story writing or brainstorming.\n",
        "\n",
        "**Combining with Temperature:**\n",
        "\n",
        "`top_k` can be used in conjunction with temperature. Temperature smooths out the distribution within the top `k` tokens, while `top_k` controls the range of tokens considered.\n",
        "\n",
        "**In your Hugging Face pipeline:**\n",
        "\n",
        "You can set the `top_k` parameter when initializing your text generation pipeline:\n",
        "\n",
        "Experiment with different `top_k` values, along with temperature, to find the optimal balance between coherence and diversity for your specific use case.\n"
      ],
      "metadata": {
        "id": "E3hEIcwVRzvt"
      }
    },
    {
      "source": [
        "# from transformers import pipeline\n",
        "# from langchain.chains import LLMChain\n",
        "# from langchain.prompts import PromptTemplate\n",
        "# from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "# zsc = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "# llm = HuggingFacePipeline.from_model_id(\n",
        "#     model_id=\"facebook/bart-large-mnli\",\n",
        "#     task=\"text-generation\",\n",
        "#     pipeline_kwargs={\n",
        "#         \"max_new_tokens\": 1, # trying to force sentiment analysis onto a chat bot\n",
        "#     },\n",
        "# )\n",
        "\n",
        "# template = \"\"\"You are a helpful assistant that can classify text.\n",
        "# Here is the text: {human_message}\n",
        "# Classification results: {classification_results}\n",
        "# \"\"\"\n",
        "# prompt = PromptTemplate(\n",
        "#     input_variables=[\"human_message\", \"classification_results\"], template=template\n",
        "# )\n",
        "\n",
        "# chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# human_message = \"This is really cool\"\n",
        "# classification_results = zsc(human_message, candidate_labels=['positive', 'negative'])\n",
        "\n",
        "# output = chain.run({\n",
        "#     \"human_message\": human_message,\n",
        "#     \"classification_results\": classification_results\n",
        "# })\n",
        "\n",
        "# print(output)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "8XUEGNSvLlzp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}