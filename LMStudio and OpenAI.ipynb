{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5de59189-75a8-411c-ac11-385750c74578",
   "metadata": {},
   "source": [
    "# LMStudio and OpenAI Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1580539c-d39c-4727-aebb-e0aa1f58e0df",
   "metadata": {},
   "source": [
    "## Code Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5812ba4b-bf67-4fed-9225-3dedc72c57b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05191153-d898-4f32-9d2e-500fbb5e3d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: reuse your existing OpenAI setup\n",
    "from openai import OpenAI\n",
    "\n",
    "# Point to the local server\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"Always answer in rhymes.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Introduce yourself.\"}\n",
    "  ],\n",
    "  temperature=0.7,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e60fdea-b3f3-41dd-9a8c-497b9abe0927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LM Studio Response:\n",
      "Once upon a time, in the quaint little village of Willowbrook, there was a dog named Max. Max was no ordinary dog; he had the extraordinary ability to speak human language fluently. He was a large, shaggy golden retriever with warm brown eyes that held an uncanny intelligence.\n",
      "\n",
      "Max lived with the Johnson family, who had adopted him as a puppy. The Johnsons were kind and loving people, but they had a young daughter named Emily who was allergic to dogs. So Max spent most of his days outside in the backyard, where he would entertain himself by playing fetch with his favorite stick or rolling in the grass.\n",
      "\n",
      "One sunny afternoon, as\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "try:\n",
    "    client = openai.OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "    # Example completion request\n",
    "    response = client.completions.create(\n",
    "        model=\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\",\n",
    "        prompt=\"Write a short story about a talking dog.\",\n",
    "        max_tokens=150,\n",
    "    )\n",
    "\n",
    "    print(\"LM Studio Response:\")\n",
    "    print(response.choices[0].text.strip())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to LM Studio: {e}\")\n",
    "    print(\"Trying OpenAI API Key...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "285c1924-8b35-42a5-a6fd-0a66914880c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chat Completion Response:\n",
      "Large language models, such as those based on deep learning techniques, offer several benefits over smaller models when it comes to natural language processing (NLP) tasks. Here are some of the key advantages:\n",
      "\n",
      "1. Improved Accuracy and Understanding: Large language models can process and understand complex contexts more effectively due to their larger size and more extensive training data. They can handle longer texts, idiomatic expressions, and nuanced meanings that smaller models might struggle with.\n",
      "\n",
      "2. Enhanced Flexibility and Adaptability: Large language models are more flexible and adaptable to various NLP tasks, such as text generation, question answering, sentiment analysis, and machine translation. They can learn to generate human-like responses, understand multiple meanings of a word based on context, and provide accurate translations between languages.\n",
      "\n",
      "3. Better Contextual Understanding: Large language models are better at understanding the context in which words and phrases are used. They can identify the relationships between different parts of a sentence or text, making them more effective for tasks like summarization, paraphrasing, and information extraction.\n",
      "\n",
      "4. Increased Creativity and Imagination: Large language models can generate creative and imaginative responses to prompts, as they have learned to understand and mimic human-like patterns of speech and thought. They can write stories, poems, or even compose music based on given inputs.\n",
      "\n",
      "5. Improved Efficiency and Scalability: Large language models are more efficient and scalable than smaller models when dealing with large volumes of data. They can process vast amounts of text quickly and accurately, making them ideal for applications like content moderation, customer support, or information retrieval in large organizations.\n",
      "\n",
      "Overall, the benefits of using large language models include improved accuracy, enhanced flexibility, better contextual understanding, increased creativity, and greater efficiency and scalability. These advantages make large language models valuable tools for a wide range of NLP applications, from everyday tasks like text summarization and translation to more complex applications like creative writing or scientific research.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"What are the benefits of using large language models?\"}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(\"LM Studio Response:\")\n",
    "    print(response.choices[0].message.content.strip())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Chat Completion Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea403ad-67a8-4639-8319-d69fef6326bb",
   "metadata": {},
   "source": [
    "## Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ca39a3-6e49-4659-b55b-1499e46cb911",
   "metadata": {},
   "source": [
    "\r\n",
    "```python\r\n",
    "from openai import OpenAI\r\n",
    "```\r\n",
    "\r\n",
    "* **`from openai import OpenAI`**: This line imports the `OpenAI` class from the `openai` library.  This class is used to interact with OpenAI's APIs (and, in this case, LM Studio which mimics the OpenAI API).\r\n",
    "\r\n",
    "```python\r\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\r\n",
    "```\r\n",
    "\r\n",
    "* **`client = OpenAI(...)`**: This creates an instance of the `OpenAI` class and assigns it to the variable `client`. This `client` object will be used to make API calls.\r\n",
    "* **`base_url=\"http://localhost:1234/v1\"`**: This argument tells the `OpenAI` client to connect to a custom base URL.  In your case, it's pointing to `http://localhost:1234/v1`. This is the address where LM Studio is running and serving its API.  LM Studio is designed to be compatible with the OpenAI API, so you can use the `openai` library to talk to it.\r\n",
    "* **`api_key=\"lm-studio\"`**:  This sets the API key.  While LM Studio *might* require an API key, it often doesn't for local use. The value \"lm-studio\" is often used as a placeholder when working with LM Studio locally, but it's not a real API key.  If LM Studio requires authentication, you'd put the real key here.\r\n",
    "\r\n",
    "```python\r\n",
    "try:\r\n",
    "    response = client.chat.completions.create(\r\n",
    "        model=\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\",\r\n",
    "        messages=[\r\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\r\n",
    "            {\"role\": \"user\", \"content\": \"What are the benefits of using large language models?\"}\r\n",
    "        ]\r\n",
    "    )\r\n",
    "    # ... (rest of the try block)\r\n",
    "except Exception as e:\r\n",
    "    print(f\"Chat Completion Error: {e}\")\r\n",
    "```\r\n",
    "\r\n",
    "* **`try:`**: This block starts a `try...except` structure.  This is used for error handling. The code within the `try` block is the code that might raise an exception (error).\r\n",
    "* **`response = client.chat.completions.create(...)`**: This is the core line that makes the API call to LM Studio.\r\n",
    "    * **`client.chat.completions.create()`**: This calls the `chat.completions.create()` method on the `client` object. This method is specifically designed for chat-style interactions.\r\n",
    "    * **`model=\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\"`**: This specifies the language model to use.  `\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\"` is a model identifier.  It looks like a Hugging Face model name, which LM Studio can likely load. Make sure this model is actually available in your LM Studio setup.\r\n",
    "    * **`messages=[...]`**: This is a list of messages that define the conversation.  Each message is a dictionary with two keys:\r\n",
    "        * **`\"role\"`**:  Indicates the role of the message sender.  It can be `\"system\"`, `\"user\"`, or `\"assistant\"`.\r\n",
    "        * **`\"content\"`**: The actual text of the message.\r\n",
    "        * `{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}`: This is the *system message*. It sets the initial behavior of the assistant.  It tells the model to act as a helpful assistant.\r\n",
    "        * `{\"role\": \"user\", \"content\": \"What are the benefits of using large language models?\"}`: This is the *user message*. It's the actual question or instruction you're giving to the model.\r\n",
    "* **`print(\"LM Studio Response:\")`**: This line prints a label to the console.\r\n",
    "* **`print(response.choices[0].message.content.strip())`**: This line processes and prints the response from the model.\r\n",
    "    * **`response.choices`**: The API can sometimes return multiple possible responses (choices).  Here, we're taking the first choice (`[0]`).\r\n",
    "    * **`.message.content`**: This accesses the actual text content of the assistant's reply.\r\n",
    "    * **`.strip()`**: This removes any leading or trailing whitespace from the response, making it cleaner to print.\r\n",
    "* **`except Exception as e:`**: This is the `except` block of the `try...except` structure. If any exception (error) occurs within the `try` block, the code in this block will be executed.\r\n",
    "* **`print(f\"Chat Completion Error: {e}\")`**: This prints an error message to the console, including the details of the exception (`e`).  This helps in debugging.\r\n",
    "\r\n",
    "In summary, this code sends a chat completion request to LM Studio, asking about the benefits of large language models. It uses a specific model (`TheBloke/Mistral-7B-Instruct-v0.2-GGUF`) and includes a system message to set the assistant's behavior. The code handles potential errors during the API call and prints the assistant's response (or the error message).  Remember to confirm that the model name is correct and available in your LM Studio environment.\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
